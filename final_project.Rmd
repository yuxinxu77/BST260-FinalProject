---
title: "BST260 Final Project"
author: "Junyi Guo"
date: "12/3/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## BST260 FINAL PROJECT {.tabset .tabset-fade}

# Overview and Motivation
Diabetes is a group of metabolic disorders characterized by a high blood sugar level over a prolonged period of time. Diabetes prevalence has been rising more rapidly among adults over 18 years of age from 4.7% in 1980 to 8.5% in 2014. On the one hand, the onset of type 2 diabetes is highly associated with an unhealthy lifestyle. On the other hand, diabetes is a major cause of various diseases and was the seventh leading cause of death in 2016. (data from WHO)
Given this situation, our motivation for this project is: first, to find and analyze possible risk factors of diabetes, such as diet, smoking, health condition, and demographic and socioeconomic factors. Based on our results, we could encourage people to follow healthier lifestyles and therefore to delay or avoid the onset of type 2 diabetes. Secondly, we would like to study the potential health-related influences of diabetes according to their lab test results and questionnaire answers. Finally, we would like to forecast whether an individual would develop Diabetes given the current living condition. We hope that these findings can lead to a better understanding of diabetes and help physicians and patients to prevent, treat, control, and manage diabetes.  

# Related Work
After we decided to do a project revolving around diabetes, we looked through some papers related to the topic, and found [one](https://bmcendocrdisord.biomedcentral.com/articles/10.1186/s12902-019-0436-6) constructing predictive models for diabetes diagnosis. It occurred to us that we want to try out something similar.

# Initial Question
We are interested in the following question:
* Do diabetes patients above age 20 tend to follow a specific demographic pattern?
* Do diabetes patients above age 20 tend to follow a specific socioeconomic pattern?
* Do diabetes patients above age 20 tend to follow a specific diet pattern(logically, they should, but do * they really follow)?
* Given certain information on demographics, socioeconomic status and diet pattern, how likely can we correctly predict whether patients above age 20 have been diagnosed with diabetes or not?


# Data Source and Data Preprocessing

We looked for relevant data on diabetes and found the dataset [`National Health and Nutrition Examination Survey`](https://www.cdc.gov/nchs/nhanes/about_nhanes.htm) collection by CDC from 2017 to 2018. It contains more than ten thousand interviewees' data that cover a variety of topics: demographics, examination, health status, diet, lab results, etcetera. We used a specific subset of the data that contains 8897 interviewees who participated in the diabetes survey. After some preprocessing and dropping NA rows and certain age groups, our explorative analysis and model training eventually ends up with a dataset of 3811 individuals.

```{r}
library(SASxport)
library(dplyr)
library(tidyverse)
library(ggcorrplot)
```
Our dataset contains a target variable in the form of a single-choice questionnaire answer, and features from four categories: diet, demographics, examination and lab.

There are a few steps that we performed across all categories of data:

1. Import data from .XPT file.
2. Select features we want and rename their columns from special indices into column names that's more intuitive.
3. Drop rows with NA values.

But there are still some steps that are special to specific categories that we went through:

## Diet Data Preprocessing
```{r}
# read in datasets
diet_1 <- read.xport('./data/DR1TOT_J.XPT')
diet_2 <- read.xport('./data/DR2TOT_J.XPT')
dat_t <- read.xport("./data/DIQ_J.XPT")
```

```{r}
# select useful columns and rename column names
diet_1 <- diet_1 %>% select(SEQN, DR1TKCAL, DR1TCARB, DR1TSUGR, DR1TTFAT, DR1TSODI) %>% rename('energy1' = 'DR1TKCAL', 'carbonhydrate1' = 'DR1TCARB', 'total_sugar1' = 'DR1TSUGR', 'total_fat1' = 'DR1TTFAT', 'sodium1' = 'DR1TSODI')
diet_2 <- diet_2 %>% select(SEQN, DR2TKCAL, DR2TCARB, DR2TSUGR, DR2TTFAT, DR2TSODI) %>% rename('energy2' = 'DR2TKCAL', 'carbonhydrate2' = 'DR2TCARB', 'total_sugar2' = 'DR2TSUGR', 'total_fat2' = 'DR2TTFAT', 'sodium2' = 'DR2TSODI')
dat_t <- dat_t %>% select(SEQN, DIQ010) %>% rename('diabetes' = 'DIQ010') %>% drop_na()

# now we can go ahead and full_join diet_1 and diet_2
dat <- full_join(diet_1, diet_2, by = 'SEQN')

# I decide to use the mean value of the two days if data from both date exist,
# use the value of one day if only data from that day exist,
# and delete the rows with all empty values except for id column.

dat <- dat %>% mutate(energy = ifelse((!is.na(energy1)) & (!is.na(energy2)), (energy1 + energy2)/2.0, ifelse(is.na(energy1), energy2, energy1)))
dat <- dat %>% mutate(carbonhydrate = ifelse((!is.na(carbonhydrate1)) & (!is.na(carbonhydrate2)), (carbonhydrate1 + carbonhydrate2)/2.0, ifelse(is.na(carbonhydrate1), carbonhydrate2, carbonhydrate1)))
dat <- dat %>% mutate(total_sugar = ifelse((!is.na(total_sugar1)) & (!is.na(total_sugar2)), (total_sugar1 + total_sugar2)/2.0, ifelse(is.na(total_sugar1), total_sugar2, total_sugar1)))
dat <- dat %>% mutate(total_fat = ifelse((!is.na(total_fat1)) & (!is.na(total_fat2)), (total_fat1 + total_fat2)/2.0, ifelse(is.na(total_fat1), total_fat2, total_fat1)))
dat <- dat %>% mutate(sodium = ifelse((!is.na(sodium1)) & (!is.na(sodium2)), (sodium1 + sodium2)/2.0, ifelse(is.na(sodium1), sodium2, sodium1)))

dat <- dat %>% select(SEQN, energy, carbonhydrate, total_sugar, total_fat, sodium) %>% drop_na()

# now dat only has 7495 rows, less than the original 8704 rows by 1209 rows
# that's exactly the expected number of empty rows

dat <- inner_join(dat,dat_t, by = 'SEQN')
dat <- dat %>% filter(diabetes == 1 | diabetes == 2)
```

Since in diet data, all wanted features may have both data from day 1 and day 2, we replace the values with the mean from the two days if both exists, and keep the value where on data from one of the day exists.


## Target data

For target data, we filtered the dataset to make sure that the target variable ("If doctors told the participant in the patients in the past that they have diabetes") only contains factors that represent yes and no.

## Demographic Data Preprocessing
```{r}
demo <- read.xport("./data/DEMO_J.XPT")
demo_clean <- demo %>% select(SEQN, DMDEDUC2, DMDMARTL, INDFMIN2, INDFMPIR,
                              RIAGENDR, RIDAGEYR, RIDRETH3) %>%
                        rename(highest_edu = DMDEDUC2, marital_status = DMDMARTL,
                               total_family_income = INDFMIN2, income_vs_poverty = INDFMPIR,
                               gender = RIAGENDR, age = RIDAGEYR, race = RIDRETH3)

# Assgin marital status with values 77-Refused, 99-Don't Know, and NA to 0-Not Specified
demo_clean <- demo_clean %>% mutate(marital_status =
                                     ifelse(marital_status == 77 | marital_status == 99 | is.na(marital_status) == TRUE, 0, marital_status))
# Drop any rows with NULL values in total_family_income, income_vs_poverty, household_income, highest_edu
demo_clean <- demo_clean %>% filter(is.na(total_family_income) != TRUE
                                    & is.na(income_vs_poverty) != TRUE
                                    & is.na(highest_edu) != TRUE)
summary(demo_clean)
# Join demographics data with other data and drop any rows with NULL values
# 5690 observation left
data <- left_join(demo_clean, dat, by = "SEQN") %>% drop_na()

```

1. In order to decrease the number of categories inside feature "marital status", we reassign answers representing "Refused", "Don't know" and NA values to the category "Not Specified".
2. To simplify of our classification problem, we decided to delete rows where feature "highest education"is only that only rarely appear in our dataset.


## Examination Data Preprocessing
```{r}
# Add BMI to the cleaned data and drop any rows with NULL values
# 5631 observation left
exam <- read.xport("./data/BMX_J.XPT")
data <- left_join(data, exam %>% select(SEQN, BMXBMI), by = "SEQN") %>%
                    rename(BMI = BMXBMI) %>%
                    drop_na()
```

## Lab Data Preprocessing
```{r}
# Add HDL_Cholesterol to the cleaned data and drop any rows with NULL values
# 5205 observation left
data$SEQN <- as.numeric(data$SEQN)
lab3 <- read.xport("./data/HDL_J.XPT")
lab3$SEQN <- as.numeric(lab3$SEQN)
data <- left_join(data, lab3 %>% select(SEQN, LBDHDD), by = "SEQN") %>%
  rename(HDL_Cholesterol = LBDHDD) %>%
  drop_na()
```

## Reassign Diabetes value
```{r}
#revalue "no diabetes" from 2 to 0
data$diabetes[data$diabetes==2]<-0
```

## Remove extremely infrequent categorical value
```{r}
data <- data %>% filter(highest_edu != 7 & highest_edu != 9 & marital_status != 0)
```


# Data Visualization

We performed some explorative analysis over the features we selected to examine if they seem to relate to our target variable (diabetes y/n). We first looked at the categorical features.


## Pie Chart
According to pie charts, we found that the distributions of some features among people with/without diabetes are quite different.


```{r}
#pie chart
pie_chart<- function(diab, cat, level) {
  pie_dat <- as.data.frame(table(data %>% filter(diabetes ==diab)%>%select(cat)))%>%rename(categories = Var1)
  pie_dat $ categories<- level
  ggplot(pie_dat, aes(x="", y=Freq, fill=categories)) +
    geom_bar(stat="identity", width=1) +
    coord_polar("y", start=0)
}

cats <- c("hightest_edu", "marital_status", "gender", "race", "total_family_income")
level_edu <- c("Less than 9th grade","9-11th grade (Includes 12th grade with no diploma)",
               "High school graduate/GED or equivalent",
               "Some college or AA degree",
               "College graduate or above")
level_marital <- c("Married", "Widowed", "Divorced", "Separated", "Never married",
                   "Living with partner")
level_gender <- c("Male", "Female")

level_race <- c("Mexican American",		
                "Other Hispanic",
                "Non-Hispanic White",
                "Non-Hispanic Black",
                "Non-Hispanic Asian",
                "Other Race - Including Multi-Racial")
level_income <- c("$0 to $4,999",
                  "$5,000 to $9,999",
                  "$10,000 to $14,999",
                  "$15,000 to $19,999",
                  "$20,000 to $24,999",
                  "$25,000 to $34,999",
                  "$35,000 to $44,999",
                  "$45,000 to $54,999",
                  "$55,000 to $64,999",
                  "$65,000 to $74,999",
                  "$75,000 to $99,999",
                  "$100,000 and Over")

###### Questions ##########
pie_chart(0, "highest_edu", level_edu)
pie_chart(1,"highest_edu", level_edu[-6])
```

First, for the highest education level, people with diabetes tend to have lower educational background than the other group. A greater fraction of people with diabetes have education levels: less than 9th grade, 9-11th grade, and high school graduate, than that of the non-diabetes group, while the non-diabetes group has more college graduate or above education background.

```{r}
pie_chart(0, "marital_status", level_marital)
pie_chart(1, "marital_status", level_marital[-7])
```

For marital status, more people are married or divorced in the diabetes group compared to the non-diabetes group. However, since marital status highly depends on age, and age is known to be a key risk factor for diabetes, this factor could be a confounder.

```{r}
pie_chart(0, "gender", level_gender)
pie_chart(1, "gender", level_gender)
```

The pie charts of gender show an interesting point. While females are slightly more than males in the non-diabetes group, more males appear in the diabetes group. This may tell us that males have a higher risk of diabetes than females.

```{r}
pie_chart(0, "race", level_race)
pie_chart(1, "race", level_race)
```

The distributions of races in these two groups are very similar, except that the diabetes group has more Mexican Americans and less Non-Hispanic Whites. Overall, race seems not to be a key feature for predicting diabetes.

```{r}
pie_chart(0, "total_family_income", level_income)
pie_chart(1, "total_family_income", level_income)
```
Lastly, looking at total family income, we might find that the fraction of people with the highest income ($100,000 and over, $75,000 to $99,999, and $65,000 to $74,999) in the non-diabetes group is greater than the diabetes group.


## Back to Back Histogram
### Age
```{r}
# back to back histogram: age
level_age <- c("[0-10)","[10-20)", "[20-30)", "[30-40)", "[40-50)", "[50-60)","[60-70)", "[70-80)","[80-90)", "[90-100)")
d_age <- data %>% select(age, diabetes) %>%
  mutate(age_tag = case_when(
    age < 10 ~ level_age[1],
    age >= 10 & age < 20 ~ level_age[2],
    age >= 20 & age < 30 ~ level_age[3],
    age >= 30 & age < 40 ~ level_age[4],
    age >= 40 & age < 50 ~ level_age[5],
    age >= 50 & age < 60 ~ level_age[6],
    age >= 60 & age < 70 ~ level_age[7],
    age >= 70 & age < 80 ~ level_age[8],
    age >= 80 & age < 90 ~ level_age[9])) %>%
  group_by(age_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
age <- d_age %>%
  ggplot(aes(x = age_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_age) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Age (years)", y = "Percentage", title = "Age-Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))

```

### Energy
```{r}
library(patchwork)

# back to back histogram: energy
level_energy <- c("[0-1000)","[1000-2000)", "[2000-3000)", "[3000-4000)", "[4000-5000)", "[5000-6000)","[6000-7000)", "[7000-8000)","[8000-9000)")
d_energy <- data %>% select(energy, diabetes) %>%
  mutate(energy_tag = case_when(
    energy < 1000 ~ level_energy[1],
    energy >= 1000 & energy < 2000 ~ level_energy[2],
    energy >= 2000 & energy < 3000 ~ level_energy[3],
    energy >= 3000 & energy < 4000 ~ level_energy[4],
    energy >= 4000 & energy < 5000 ~ level_energy[5],
    energy >= 5000 & energy < 6000 ~ level_energy[6],
    energy >= 6000 & energy < 7000 ~ level_energy[7],
    energy >= 7000 & energy < 8000 ~ level_energy[8],
    energy >= 8000 & energy < 9000 ~ level_energy[9])) %>%
  group_by(energy_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
energy <- d_energy %>%
  ggplot(aes(x = energy_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_energy) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Energy (kcal)", y = "Percentage", title = "Daily Energy Intake \n Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))

```

### Carbonhydrate
```{r}
# back to back histogram: carbonhydrate
level_carb <- c("[0-100)","[100-200)", "[200-300)", "[300-400)", "[400-500)", "[500-600)","[600-700)", "[700-800)","[800-900)", "[900-1000)", "[1000-1100)")
d_carb <- data %>% select(carbonhydrate, diabetes) %>%
  mutate(carb_tag = case_when(
    carbonhydrate < 100 ~ level_carb[1],
    carbonhydrate >= 100 & carbonhydrate < 200 ~ level_carb[2],
    carbonhydrate >= 200 & carbonhydrate < 300 ~ level_carb[3],
    carbonhydrate >= 300 & carbonhydrate < 400 ~ level_carb[4],
    carbonhydrate >= 400 & carbonhydrate < 500 ~ level_carb[5],
    carbonhydrate >= 500 & carbonhydrate < 600 ~ level_carb[6],
    carbonhydrate >= 600 & carbonhydrate < 700 ~ level_carb[7],
    carbonhydrate >= 700 & carbonhydrate < 800 ~ level_carb[8],
    carbonhydrate >= 800 & carbonhydrate < 900 ~ level_carb[9],
    carbonhydrate >= 900 & carbonhydrate < 1000 ~ level_carb[10],
    carbonhydrate >= 1000 & carbonhydrate < 1100 ~ level_carb[11])) %>%
  group_by(carb_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
carb <- d_carb %>%
  ggplot(aes(x = carb_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_carb) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Carbonhydrate (gm)", y = "Percentage", title = "Daily Carbonhydrate Intake \n Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))

```

### Sugar
```{r}
# back to back histogram: sugar
level_sugar <- c("[0-100)","[100-200)", "[200-300)", "[300-400)", "[400-500)", "[500-600)","[600-700)", "[700-800)","[800-900)", "[900-1000)")
d_sugar <- data %>% select(total_sugar, diabetes) %>%
  mutate(sugar_tag = case_when(
    total_sugar < 100 ~ level_sugar[1],
    total_sugar >= 100 & total_sugar < 200 ~ level_sugar[2],
    total_sugar >= 200 & total_sugar < 300 ~ level_sugar[3],
    total_sugar >= 300 & total_sugar < 400 ~ level_sugar[4],
    total_sugar >= 400 & total_sugar < 500 ~ level_sugar[5],
    total_sugar >= 500 & total_sugar < 600 ~ level_sugar[6],
    total_sugar >= 600 & total_sugar < 700 ~ level_sugar[7],
    total_sugar >= 700 & total_sugar < 800 ~ level_sugar[8],
    total_sugar >= 800 & total_sugar < 900 ~ level_sugar[9],
    total_sugar >= 900 & total_sugar < 1000 ~ level_sugar[10])) %>%
  group_by(sugar_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
sugar <- d_sugar %>%
  ggplot(aes(x = sugar_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_sugar) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Total Sugar (gm)", y = "Percentage", title = "Daily Sugar Intake \n Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))

```

### Fat
```{r}
# back to back histogram: fat
level_fat <- c("[0-50)","[50-100)", "[100-150)", "[150-200)", "[200-250)", "[250-300)","[300-350)", "[350-400)")
d_fat <- data %>% select(total_fat, diabetes) %>%
  mutate(fat_tag = case_when(
    total_fat < 50 ~ level_fat[1],
    total_fat >= 50 & total_fat < 100 ~ level_fat[2],
    total_fat >= 100 & total_fat < 150 ~ level_fat[3],
    total_fat >= 150 & total_fat < 200 ~ level_fat[4],
    total_fat >= 200 & total_fat < 250 ~ level_fat[5],
    total_fat >= 250 & total_fat < 300 ~ level_fat[6],
    total_fat >= 300 & total_fat < 350 ~ level_fat[7],
    total_fat >= 350 & total_fat < 400 ~ level_fat[8])) %>%
  group_by(fat_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
fat <- d_fat %>%
  ggplot(aes(x = fat_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_fat) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Total Fat (gm)", y = "Percentage", title = "Daily Fat Intake \n Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))

```

### Sodium
```{r}
# back to back histogram: sodium
level_sodium <- c("[0-2000)","[2000-4000)", "[4000-6000)", "[6000-8000)", "[8000-10000)", "[10000-12000)","[12000-14000)", "[14000-16000)","[16000-18000)", "[18000-20000)", "[20000-22000)", "[22000-24000)", "[24000-26000)")
d_sodium <- data %>% select(sodium, diabetes) %>%
  mutate(sodium_tag = case_when(
    sodium < 2000 ~ level_sodium[1],
    sodium >= 2000 & sodium < 4000 ~ level_sodium[2],
    sodium >= 4000 & sodium < 6000 ~ level_sodium[3],
    sodium >= 6000 & sodium < 8000 ~ level_sodium[4],
    sodium >= 8000 & sodium < 10000 ~ level_sodium[5],
    sodium >= 10000 & sodium < 12000 ~ level_sodium[6],
    sodium >= 12000 & sodium < 14000 ~ level_sodium[7],
    sodium >= 14000 & sodium < 16000 ~ level_sodium[8],
    sodium >= 16000 & sodium < 18000 ~ level_sodium[9],
    sodium >= 18000 & sodium < 20000 ~ level_sodium[10],
    sodium >= 20000 & sodium < 22000 ~ level_sodium[11],
    sodium >= 22000 & sodium < 24000 ~ level_sodium[12],
    sodium >= 24000 & sodium < 26000 ~ level_sodium[13])) %>%
  group_by(sodium_tag, diabetes) %>%
  summarise(percentage = n(), .groups = "drop") %>%
  mutate(percentage = ifelse(diabetes == 1, -percentage/sum(data$diabetes == 1), percentage/sum(data$diabetes == 0))) %>%
  mutate(diabetes = ifelse(diabetes == 0, "No Diabetes", "Has Diabetes"))

# plot
sodium <- d_sodium %>%
  ggplot(aes(x = sodium_tag, y = percentage, group = diabetes, fill = diabetes)) +
  geom_bar(stat = "identity", width = 0.75) +
  coord_flip() +
  scale_x_discrete(limits = level_sodium) +
  # another trick!
  scale_y_continuous(limits = c(-1, 1),
                     breaks = seq(-1, 1, 0.1),
                     labels = abs(seq(-1 , 1, 0.1))) +
  labs(x = "Total Sodium (mg)", y = "Percentage", title = "Daily Sodium Intake \n Diabetes Distribution Comparison") +
  theme(legend.position = "bottom",
        legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5),
        panel.background = element_rect(fill =  "grey90")) +
  scale_fill_manual(values=c("red", "blue"),
                    name="",
                    breaks=c("Has Diabetes", "No Diabetes"),
                    labels=c("Has Diabetes", "No Diabetes"))
```
```{r}
print(age)
```
According to CDC, more than 90% American diabetes patient has type two diabetes, which most often develops in people more than age 45 ([reference](https://www.cdc.gov/diabetes/basics/type2.html)). Our data accurately captured this skew in age distribution among diabetes patients: patients above age 40 takes up 95% of the whole population of positively diagnosed patients. In the contrary, the distribution of the healthy population spreads more evenly between 20 and 70, and only drastically decreased in the age group 70 or above. This distribution makes sense, since the elderly population might be less responsive to surveys compared to younger age groups.

```{r}
print(energy)
print(carb)
print(sugar)
print(fat)
print(sodium)
```

In terms of diet distribution, patients diagnosed with diabetes don't differ much from the healthy population. But there is one consistent pattern across the five graphs: diagnosed patients in general tend to consume less compared to the healthy. Given that all diabetes patients already know there diagnosis and will thus pay more attention to their daily consumption, this difference is reasonable.

## Histogram
```{r}
# data %>% select(total_family_income, diabetes) %>%
#     mutate(total_family_income = as.numeric(total_family_income), diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
#     ggplot(aes(x=total_family_income, color=diabetes, fill=diabetes)) +
#     geom_histogram(aes(y=..density..), position="identity", alpha=0.5, bins = 15) +
#     geom_density(alpha=0.6) +
#     scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
#     scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
#     labs(title="Total Family Income",x="Total Family Income", y = "Density")+
#     theme_classic()
```

### Income vs Poverty
```{r}
mean_ip <- data %>% select(income_vs_poverty, diabetes) %>%
    mutate(diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    group_by(diabetes) %>%
    summarise(mean = mean(income_vs_poverty), .groups = "drop")

data %>% select(income_vs_poverty, diabetes) %>%
    mutate(diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    rename(Diabetes = diabetes) %>%
    ggplot(aes(x=income_vs_poverty, color=Diabetes, fill=Diabetes)) +
    geom_histogram(aes(y=..density..), position="identity", alpha=0.5, bins = 15) +
    geom_density(alpha=0.4) +
    geom_vline(data = mean_ip, aes(xintercept=mean, color=diabetes),
             linetype="dashed")+
    scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    labs(title="Ratio of Family Income to Poverty",x="Family Income Ratio", y = "Density")+
    theme_classic()
```
The difference between distribution of diabetes patients and that of the healthy is not big, but in general, diabetes patients tend to have less family income compared to the healthy. Considering the fact that illness might potentially decrease patients' ability of making money, the difference is consistent to the reality.


### BMI
```{r}
mean_bmi <- data %>% select(BMI, diabetes) %>%
    mutate(diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    group_by(diabetes) %>%
    summarise(mean = mean(BMI), .groups = "drop")

data %>% select(BMI, diabetes) %>%
    mutate(diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    rename(Diabetes = diabetes) %>%
    ggplot(aes(x=BMI, color=Diabetes, fill=Diabetes)) +
    geom_histogram(aes(y=..density..), position="identity", alpha=0.5, bins = 20) +
    geom_density(alpha=0.4) +
    geom_vline(data = mean_bmi, aes(xintercept=mean, color=diabetes),
             linetype="dashed")+
    scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    labs(title="Body Mass Index",x="Body Mass Index (kg/m**2)", y = "Density")+
    theme_classic()
```

The difference between Body Mass Index distributions of diabetes patients and the healthy is still small, but BMI distribution is more right skewed compared to that of the healthy, implying a higher BMI. This is consistent with the fact that BMI is positively correlated with risk of diabetes ([reference](https://care.diabetesjournals.org/content/30/6/1562#:~:text=Lifetime%20risk%20by%20BMI,%3E35%20kg%2Fm2.)).

### HDL_Cholesterol
```{r}
mean_cho <- data %>% select(HDL_Cholesterol, diabetes) %>%
    mutate(diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    group_by(diabetes) %>%
    summarise(mean = mean(HDL_Cholesterol), .groups = "drop")

data %>% select(HDL_Cholesterol, diabetes) %>%
    mutate(HDL_Cholesterol = as.numeric(HDL_Cholesterol),
           diabetes = ifelse(diabetes == 1, "Diabetes", "No Diabetes")) %>%
    rename(Diabetes = diabetes) %>%
    ggplot(aes(x=HDL_Cholesterol, color=Diabetes, fill=Diabetes)) +
    geom_histogram(aes(y=..density..), position="identity", alpha=0.5, bins = 20) +
    geom_density(alpha=0.4) +
    geom_vline(data = mean_cho, aes(xintercept=mean, color=diabetes),
             linetype="dashed")+
    scale_color_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    scale_fill_manual(values=c("#999999", "#E69F00", "#56B4E9"))+
    labs(title="Direct HDL-Cholesterol",x="Direct HDL-Cholesterol (mg/dL)", y = "Density")+
    theme_classic()
```
According to the American Heart Association, diabetes patients tend to have a higher cholesterol levels ([reference](https://www.heart.org/en/health-topics/diabetes/why-diabetes-matters/cholesterol-abnormalities--diabetes)). This fact is reflected in oure data: the Cholesterol distribution of diabetes patients in general is more right-skewed than that of the healthy.

## Correlation Plot
```{r}
#correlation plot
r <- cor(data, use="complete.obs")
ggcorrplot(r)
```

We then constructed a heatmap to inspect the correlation between features. Not to our surprise, features from the diet category are highly correlated (which makes sense, since if people eat more or less, the amount of their carbonhydrate, sugar, sodium and fat intake will likely increase or decrease accordingly). On the other hand, the high correlation between total family income and income-poverty ratio is also understandable, given that they are likely just a multiple of each other. There are also some other explainable correlations displayed in the heatmap: the correlation between age and marital status, between highest education degree and income-poverty ratio, between daily energy intake and gender, etcetera.


# Building Data Pipeline
## One-hot encoding

We then use one-hot encoding to convert categorical features into multiple columns that eliminates the unnecessary ordinality between categeories.

```{r}
library(mltools)
library(data.table)

data_onehot <- data %>% select(-c("SEQN")) %>%
    mutate(total_family_income = ifelse(total_family_income == 14 | total_family_income == 15,
                                        ifelse(total_family_income == 14, 11, 12), total_family_income)) %>%
    mutate(highest_edu = as.factor(highest_edu),
           marital_status = as.factor(marital_status),
           race = as.factor(race))
data_onehot <- one_hot(data.table(data_onehot))
data_onehot <- data_onehot %>% mutate(diabetes = as.factor(diabetes))
```

## Train-test split

We then randomly split the data into training set (80%) and testing set (20%).
```{r}
# data_0 <- data_onehot %>% filter(diabetes == 0)
# data_1 <- data_onehot %>% filter(diabetes == 1)
# data_0_sample <- sample_n(data_0, nrow(data_1))
# data_sample <- rbind(data_0_sample, data_1)
#
# train_index <- createDataPartition(data_sample$diabetes, times = 1, p = 0.8, list = FALSE)
# train_set <- data_sample[train_index, ]
# test_set <- data_sample[-train_index, ]
# X_train <- train_set %>% select(-c(diabetes))
# y_train <- train_set$diabetes
# X_test <- test_set %>% select(-c(diabetes))
# y_test <- test_set$diabetes
```

```{r}
library(caret)
library(ROSE)
train_index <- createDataPartition(data_onehot$diabetes, times = 1, p = 0.8, list = FALSE)
train_set <- data_onehot[train_index, ]
train_balanced <- ovun.sample(diabetes ~ ., data = train_set, method = "over", N = 5108)$data
table(train_balanced$diabetes)

test_set <- data_onehot[-train_index, ]

# Unbalanced training data
X_train <- train_set %>% select(-c(diabetes))
y_train <- train_set$diabetes

# Balanced training data
X_train_balanced <- train_balanced %>% select(-c(diabetes))
y_train_balanced <- train_balanced$diabetes

X_test <- test_set %>% select(-c(diabetes))
y_test <- test_set$diabetes
```

```{r}
table(data$highest_edu)
table(data$marital_status)
table(data$gender)
table(data$race)
```

## PCA
```{r}
# Install the package if needed
# Run full PCA to select principle components
# install.packages("factoextra")
library(factoextra)
library("corrplot")
pca_full <- prcomp(X_train, scale = TRUE)
summary(pca_full)
fviz_eig(pca_full)
fviz_pca_var(pca_full,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )

corrplot(get_pca(pca_full)$cos2, is.corr=FALSE)

```

```{r}
# Run pca on selected number of principle components
pca_part <- prcomp(X_train, scale = TRUE, rank. = 10)
summary(pca_part)
# Apply pca transform on both training set and test set
# Unbalanced training data
X_train_pca = data.frame(pca_part$x)
X_test_pca = data.frame(predict(pca_part, newdata = X_test))
```


```{r}
# Run pca on selected number of priciple components
pca_balanced_part <- prcomp(X_train_balanced, scale = TRUE, rank. = 10)
summary(pca_balanced_part)
# Apply pca transform on both training set and test set
# Unbalanced training data
X_train_balanced_pca = data.frame(pca_balanced_part$x)
X_test_balanced_pca = data.frame(predict(pca_balanced_part, newdata = X_test))
```


```{r}
#Convert target to factor
# y_train <- as.factor(y_train)
# y_test <- as.factor(y_test)
```


# Modeling
```{r}
# Runting: Boosting, RF
# Yuxin: SVM, KNN
# Jessie: Logistic Regression, Linear Regression
```

## Linear Regression
```{r}
# LR: not balanced, no pca
library(pROC)
train_regression <- cbind(X_train, y_train) %>% rename(diabetes = y_train)
glm_fit <- glm(diabetes ~ ., data = train_regression, family=binomial)
summary(glm_fit)
glm_predict_prob <- predict(glm_fit, X_test, type="response")
glm_predict <- ifelse(glm_predict_prob >= 0.1, 1, 0)
confusionMatrix(as.factor(glm_predict), as.factor(y_test), positive = "1")
roc_curve <- roc(y_test, glm_predict_prob,
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")
```

```{r}
# LR: not balanced, no pca, cv
train_control <- trainControl(method = "cv", number = 10)
glm_cv <- train(diabetes ~ .,
               data = train_regression,
               trControl = train_control,
               method = "glm",
               family=binomial())

summary(glm_cv)
glm_predict_prob <- predict(glm_cv, X_test, type="prob")
glm_predict <- ifelse(glm_predict_prob >= 0.5, 1, 0)
glm_predict <- ifelse(glm_predict[, 1] == 1, 0, 1)
confusionMatrix(as.factor(glm_predict), as.factor(y_test), positive = "1")
roc_curve <- roc(y_test, glm_predict_prob[, 1],
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")
```

```{r}
# LR: not balanced, with PCA
# library(pROC)
train_regression_pca <- cbind(X_train_pca, y_train) %>% rename(diabetes = y_train)
glm_fit_pca <- glm(diabetes ~ ., data = train_regression_pca, family=binomial)
summary(glm_fit_pca)
glm_predict_prob <- predict(glm_fit_pca, X_test_pca, type="response")
glm_predict <- ifelse(glm_predict_prob >= 0.5, 1, 0)
confusionMatrix(as.factor(glm_predict), as.factor(y_test), positive = "1")
roc_curve <- roc(y_test, glm_predict_prob,
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")
```

```{r}
# LR: balanced, not PCA
# library(pROC)
train_regression_balanced <- cbind(X_train_balanced, y_train_balanced) %>% rename(diabetes = y_train_balanced)
glm_fit_balanced <- glm(diabetes ~ ., data = train_regression_balanced, family=binomial)
summary(glm_fit_balanced)
glm_predict_prob <- predict(glm_fit_balanced, X_test, type="response")
glm_predict <- ifelse(glm_predict_prob >= 0.37, 1, 0)
confusionMatrix(as.factor(glm_predict), as.factor(y_test), positive = "1")
roc_curve <- roc(y_test, glm_predict_prob,
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")

```

```{r}
#function to plot roc curve
plot_roc <- function(pre){
roc_curve <- roc(y_test, as.numeric(pre),
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")
}
```


```{r}
# LR: balanced, with PCA
# library(pROC)
train_regression_balanced_pca <- cbind(X_train_balanced_pca, y_train_balanced) %>% rename(diabetes = y_train_balanced)
glm_fit_balanced_pca <- glm(diabetes ~ ., data = train_regression_balanced_pca, family=binomial)
summary(glm_fit_balanced_pca)
glm_predict_prob <- predict(glm_fit_balanced_pca, X_test_balanced_pca, type="response")
glm_predict <- ifelse(glm_predict_prob >= 0.37, 1, 0)
confusionMatrix(as.factor(glm_predict), as.factor(y_test), positive = "1")
roc_curve <- roc(y_test, glm_predict_prob,
                 smoothed = TRUE,
                 # arguments for ci
                 ci=TRUE, ci.alpha=0.9, stratified=FALSE,
                 # arguments for plot
                 plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
                 print.auc=TRUE, show.thres=TRUE)
sensitivity_ci <- ci.se(roc_curve)
plot(sensitivity_ci, type="shape", col="lightblue")
```

#RF

```{r}
library(randomForest)
library(ROSE)

```

```{r}
#best rf for No PCA, balanced GOOD
rf_classifier = randomForest(x = X_train_balanced, y = y_train_balanced, xtest = X_test, ytest=y_test, ntree=100, mtry=5, importance=TRUE, keep.forest=TRUE, trControl = tr, nodesize = 5, maxnodes = 15) #maxnodes is the most important
#rf_classifier
pre_prob = predict(rf_classifier, X_test, type = "prob")[,1]
pre = ifelse(pre_prob >= 0.52, 0, 1)
confusionMatrix(as.factor(pre), as.factor(y_test))
plot_roc(pre_prob)
```
We trained Random Forest models on four training sets, with/without PCA and balanced/not balanced, respectively. After carefully tuning parameters and plotting ROC curves, we found that random forest models are very sensitive to imbalanced data. In our original training set, there are 1549 negative(0) observations, but only 500 positive(1) observations. Therefore, there is a significant probability that a bootstrap sample contains few or even none of the minority class, resulting in a tree with poor performance for predicting the minority class.(https://statistics.berkeley.edu/tech-reports/666) Indeed, two of our models, trained by original imbalanced data, can hardly discriminate between 0 and 1. Instead, they classify almost everyone into the negative class.

Furthermore, in the two models trained by balanced data, we found that the model without PCA gives a better result, possibly because some information was lost in the process of performing PCA.

Finally, from all Random Forest models, we chose the model trained by balanced data without PCA. The area under the ROC curve (AUC) is 0.83, suggesting that it is a fairly good classification model. As high sensitivity is our first goal, we set a threshold such that sensitivity is high enough while specificity is acceptable.  


# Boosting
```{r}
library(gbm)
#no pca not balanced GOOD
data_nn = X_train
class(y_train)
data_nn$y = y_train

boost=gbm(y ~.,
              data = data_nn,
              distribution = "multinomial",
              cv.folds = 10,
              shrinkage = .1,
              n.minobsinnode = 20,
              n.trees = 150) #reduce n.trees to increase sensitivity
pre <- predict(boost, newdata = X_test, type = "response", n.trees = 200)[,,1][,1]
y_pred <- ifelse(as.numeric(pre)>0.87,0,1)
confusionMatrix(as.factor(y_pred), as.factor(y_test))
plot_roc(as.numeric(pre))
```
Gradient boosting models tell a different story. Similarly, as training Random Forest model, we tried on four training datasets with/without PCA and balanced/non-balanced.
However, in this case, PCA had a strong negative impact on model performance. On the other hand, the boosting model is very robust to handle imbalanced data: both models training by balanced/imbalanced data give similar result, with AUC around 0.83. To keep the model training process simple, we chose the one trained by original imbalanced data.

##SVM

```{r}
# SVM not PCA, balanced: sensitivity 0.83, specificity 0.66, roc 0.81 Good

svm <- svm(x = X_train_balanced, y = y_train_balanced, scale = TRUE, kernel = 'linear', cross = 10, probability = TRUE)

pre_svm <- predict(svm, X_test, probability = TRUE)
# print(pre_svm)

pre_svm <- attr(pre_svm,"prob")[,2]
plot_roc(as.numeric(pre_svm))
# print(pre_svm)

pre_svm <- ifelse(pre_svm > 0.45, 1, 0)
confusionMatrix(as.factor(pre_svm), y_test, positive = "1")
```
SVM stands for support vector machine. This binary classification algorithm aims to create a hyperplane in the hyperspace of data with features being the dimensions, that split the two classes to different sides as much as possible. In our training, we used linear kernal, did 10-fold cross validation, and trained using dataset with principal component analysis and oversampling, dataset without principal component analysis and with oversampling, dataset with principal component analysis and without oversampling, and dataset without principal component analysis or oversampling. We also normalized each feature before training so that they all have zero mean and unit variance.

Turns out that the best model trained using SVM algorithm is the one trained with dataset oversampled and without principal component analysis. It has a sensitivity of 0.83, a specificity of 0.66, and a 0.81 auc-roc score.

It's not hard to see why SVM performs better in a balanced dataset using oversampling. Since SVM uses penalty to update the hyperplane at each round, if the data is significantly imbalanced, SVM is going to penalize more by the dominant class and affected less by the class with fewer data. In our case, we care more about the class with less instances (the positive class). As a result, it's easy to see that a balanced dataset will produce a better performance.

We suspect the reason that SVM performs better with principal component analysis is that PCA filtered out some spatial information in the 28 features we picked that could be important to the training of SVM.  


## KNN
```{r}
library(class)

# KNN pca, balanced: sensitivity 0.73, specificity 0.61, roc 0.67  Normal-Good

pre_knn <- knn(X_train_balanced_pca, X_test_balanced_pca , cl = y_train_balanced, k = 18, l = 0, prob = TRUE, use.all = TRUE)

pre_knn <- ifelse(pre_knn == 1, attr(pre_knn, "prob"), 1 - attr(pre_knn, "prob"))
plot_roc(as.numeric(pre_knn))

pre_knn <- ifelse(pre_knn > 0.4, 1, 0)
confusionMatrix(as.factor(pre_knn), y_test, positive = "1")

```

KNN stands for k-nearest neighbor. It assumes that similar objects belongs to the the same class. For each data point in the test set, it finds the nearest k neighbors with classes, look at their classes and assign the class with most vote to the data point. For KNN, we set k to 18, and trained using dataset with principal component analysis and oversampling, dataset without principal component analysis and with oversampling, dataset with principal component analysis and without oversampling, and dataset without principal component analysis or oversampling.

For KNN, the best model is the one trained with dataset oversampled and with principal component analysis. It has a sensitivity of 0.73, a specificity of 0.61, and a 0.67 auc-roc score.

It's also not a surprise that KNN performs better in a balanced dataset. KNN is, in its essence, a communty-level voting system that considers a majority of vote as a win. So no wonder the dominant class will use its dominance in number to affect the voting result, even when in some cases, data points have some neighbors from the non-dominant class and actually belong to that class themselves. A balanced dataset can avoid much of this negative effect.

KNN requires a dense distribution of data points in the hyperspace of data, and is thus very vulnerable to the curse of dimensionality. With principal component analysis decreasing th dimensions, for KNN, it is imaginable that this help outweighs the loss of information caused by principal component analysis.


## Oversampling vs PCA
## Shiny app
